# Архитектура Kubernetes

## Введение

В частном случае кластер Kubernetes можно развернуть на одном сервере, где будут запущены как управляющие компоненты, так и рабочие нагрузки. Также существуют архитектуры с одним мастер-узлом или одним узлом для запуска рабочих нагрузок. Однако такие кластеры не подходят для продакшн-среды, так как не обеспечивают достаточной отказоустойчивости и доступности как самого кластера, так и размещённых в нём приложений.

Далее, говоря о кластере Kubernetes, мы будем подразумевать «классическую» архитектуру, которая включает в себя несколько мастер-узлов и несколько рабочих узлов.

## Сетевое взаимодействие в Kubernetes

При работе с Kubernetes важно понимать, как устроена его сеть. В этом модуле рассмотрим:

- Как организовано сетевое взаимодействие внутри кластера.
- Как пользователи могут стабильно, безопасно и единообразно подключаться к приложениям.
- Что такое Node Network и Pod Network.
- Виды Service и их назначение.
- Что такое Container Network Interface (CNI).
- Как маршрутизируется внешний трафик.
- Как сервисы находят друг друга внутри кластера.

Но прежде чем углубляться в сеть Kubernetes, разберёмся с его архитектурой.

## Компоненты кластера Kubernetes

Kubernetes — это система автоматизации развёртывания, масштабирования и управления контейнеризированными приложениями. В её основе лежит кластер, представляющий собой набор виртуальных или физических серверов (узлов). Архитектура Kubernetes рассматривается с двух точек зрения:

1. **Программные компоненты:** какие функции выполняют основные элементы системы.
2. **Распределение компонентов:** как они размещены на серверах кластера.

Компоненты кластера делятся на две группы:

### Управляющие компоненты (Control Plane)

Control Plane управляет работой кластера. В его состав входят:

- **kube-apiserver** — основной API-сервер, который является входной точкой для всех запросов.
- **etcd** — распределённое хранилище, используемое для хранения состояния кластера.
- **kube-scheduler** — компонент, отвечающий за распределение подов по узлам.
- **kube-controller-manager** — группа контроллеров, которые следят за состоянием объектов Kubernetes.

### Компоненты узлов (Worker Nodes)

Эти компоненты обеспечивают запуск и управление подами:

- **kubelet** — агент, управляющий работой контейнеров на узле.
- **Container Runtime** — среда выполнения контейнеров (например, Docker или containerd).
- **kube-proxy** — сетевой прокси, обеспечивающий доступ подов к сервисам.

## Типы узлов в кластере

Кластер Kubernetes состоит из двух типов узлов:

1. **Worker-узлы (рабочие узлы)** — выполняют контейнеризированные приложения.
2. **Master-узлы** — управляют кластером, запуская компоненты Control Plane.

На мастер-узлах также установлены компоненты узла, поскольку некоторые элементы Control Plane разворачиваются с помощью Kubernetes. Хотя возможно запускать рабочие нагрузки на мастер-узлах, это считается плохой практикой.

Таким образом, архитектуру Kubernetes можно представить в виде следующей схемы:

![ARCHK8S](https://pictures.s3.yandex.net/resources/3.1_1_1731056294.png)

# Управляющие компоненты Kubernetes

## kube-apiserver

**API-сервер** — это ключевой компонент управляющего слоя Kubernetes, предоставляющий API-интерфейс для взаимодействия с кластером. К API-серверу обращаются как внешние клиенты (например, `kubectl`), так и внутренние компоненты кластера, которые взаимодействуют друг с другом исключительно через API.

### Основные функции:
- Реализует **авторизацию и аутентификацию**, определяя, какой пользователь обращается к кластеру и какие у него права.
- Позволяет управлять всеми объектами Kubernetes, так как всё в Kubernetes рассматривается как объект API.
- Поддерживает **горизонтальное масштабирование**, обычно разворачивается в нескольких экземплярах с балансировкой нагрузки для обеспечения отказоустойчивости.

Kubernetes использует **REST API**, а все данные хранятся в `etcd`.

## etcd

**etcd** — это распределённое хранилище типа **"ключ-значение"**, которое используется Kubernetes для хранения конфигурации и состояния кластера.

### Почему именно etcd?
- **Строгая согласованность данных** между узлами.
- **Распределённость** и высокая доступность.
- Поддержка подписки на изменения ключей, что позволяет оперативно реагировать на изменения.

В кластере `etcd` всегда есть **узел-лидер**, на который направляются все запросы на запись данных. Выбор лидера выполняется по алгоритму **RAFT**. Для корректной работы кластера `etcd` его узлы должны иметь **нечётное количество**.

## kube-controller-manager

**Controller Manager** управляет состоянием кластера, стремясь привести его к желаемому виду. За каждый тип ресурсов отвечает **отдельный контроллер**, но все они объединены в один бинарный файл `kube-controller-manager`.

### Основные функции контроллеров:
- **Node-контроллер** — следит за состоянием узлов, реагирует на их отказ.
- **Job-контроллер** — управляет одноразовыми заданиями (Jobs).
- **EndpointSlice-контроллер** — создаёт объекты `EndpointSlice` для связи сервисов и подов.
- **Deployment-контроллер** — создаёт объекты `ReplicaSet`, а `ReplicaSet-контроллер`, в свою очередь, создаёт поды.

Контроллеры постоянно взаимодействуют с API-сервером, запрашивая текущее состояние объектов и отправляя команды на изменение.

## kube-scheduler

**Scheduler** (планировщик) отвечает за назначение подов на конкретные узлы кластера. Его задача — **не просто выбрать узел**, но и **оптимально распределить нагрузку**.

### Как работает планировщик?
- Анализирует состояние узлов, их ресурсы и загруженность.
- Учитывает требования пода к ресурсам (CPU, память).
- Обрабатывает правила **affinity/anti-affinity**, которые определяют, какие поды могут или не могут размещаться вместе.

Таким образом, `kube-scheduler` играет важную роль в балансировке нагрузки и эффективности работы кластера Kubernetes.

# Компоненты узла Kubernetes

Помимо управляющих компонентов, которые обеспечивают работу всего кластера, каждый узел содержит **компоненты узла**, которые поддерживают запуск подов и обеспечивают их функционирование. В эту группу входят:

- **Container Runtime**
- **kubelet**
- **kube-proxy**

## Container Runtime

**Container Runtime** (среда выполнения контейнеров) — компонент, который позволяет Kubernetes запускать контейнеры и управлять их жизненным циклом. Без него работа контейнеров в кластере невозможна, поэтому он должен быть установлен на каждом узле.

### Популярные реализации Container Runtime:
- **Containerd**
- **Docker Engine**
- **CRI-O**
- **runc**
- **Podman**

Kubernetes поддерживает только те среды выполнения, которые соответствуют спецификации **CRI (Container Runtime Interface)**. Это обеспечивает независимость Kubernetes от конкретного контейнерного движка и позволяет администраторам выбирать удобную реализацию.

## kubelet

**kubelet** — агент, работающий на каждом узле кластера и обеспечивающий запуск и управление подами.

### Как работает kubelet?
1. Получает **спецификацию подов** от API-сервера.
2. Запускает контейнеры через **Container Runtime**.
3. Мониторит состояние запущенных контейнеров.
4. Перезапускает контейнеры при сбоях.

Важно!  
`kubelet` управляет **только теми контейнерами**, которые были запущены через Kubernetes.

Кроме API-сервера, kubelet может получать спецификацию подов из:
- **Файлов на сервере** — агент периодически проверяет указанную директорию.
- **HTTP-эндпоинтов** — kubelet запрашивает удалённый ресурс для получения описаний подов.

Дополнительно, kubelet регистрирует узел в API-сервере Kubernetes.

## kube-proxy

**kube-proxy** — сетевой прокси, обеспечивающий сетевое взаимодействие внутри кластера.

### Функции kube-proxy:
- Настраивает **правила сети** на узлах.
- Обеспечивает **доступ к подам** внутри и извне кластера.
- Реализует **натирование (NAT)**, сопоставляя IP-адреса сервисов с IP-адресами подов.

kube-proxy получает сетевые правила из API-сервера и применяет их на узле, управляя маршрутизацией трафика между сервисами и подами.

---

Эти три компонента обеспечивают корректную работу узла в Kubernetes, поддерживая выполнение подов и взаимодействие между ними.

# Процесс запуска контейнера в Kubernetes

В Kubernetes запуск приложения выглядит как магия: мы описываем желаемый результат, а платформа делает всё остальное. Однако под капотом происходит множество процессов, которые важно понимать.

Рассмотрим поэтапно, что происходит при выполнении команды:

```sh
kubectl apply -f job.yaml
```
# Процесс запуска контейнера в Kubernetes

## 1. Обращение к API-серверу

- `kubectl` использует `kubeconfig` для определения **адреса API-сервера** и **данных аутентификации**.
- Отправляется запрос на **создание объекта Job**.
- API-сервер **аутентифицирует клиента** и **проверяет права доступа**.
- API-сервер записывает объект `Job` в **etcd**.
- API-сервер сообщает `kubectl`, что `Job` успешно создан.

## 2. Действия контроллера (kube-controller-manager)

- **Job-контроллер** обнаруживает новый объект `Job`.
- Контроллер **создаёт объект Pod** на основе `podTemplate`.
- API-сервер записывает объект `Pod` в **etcd** и подтверждает создание.
- Контроллер получает подтверждение.

## 3. Действия планировщика (kube-scheduler)

- Планировщик обнаруживает новый **под без привязанного узла (`nodeName` пуст)**.
- Определяет подходящий **узел** и отправляет обновлённую информацию в API-сервер.
- API-сервер обновляет объект **Pod** в **etcd** и подтверждает изменения.

## 4. Действия kubelet на узле

- `kubelet` на узле **обнаруживает новый под**, который должен быть запущен.
- Загружает **образ контейнера** через **Container Runtime** (например, containerd, Docker).
- Запускает **контейнеры**, следит за их состоянием.
- Сообщает API-серверу о **текущем статусе** пода.
- API-сервер обновляет **состояние пода** в **etcd**.

## Мы рассмотрели основные этапы запуска контейнера в Kubernetes:

- Создание объекта через API.
- Запись данных в etcd.
- Работа контроллера, который создаёт под.
- Планирование узла с помощью kube-scheduler.
- Запуск контейнера через kubelet и Container Runtime.

# Сеть в Kubernetes

Вы научились запускать приложение — пора настраивать сетевую конфигурацию, чтобы в дальнейшем можно было запускать несколько взаимодействующих между собой приложений. В этом уроке мы рассмотрим, как **поды** и **узлы** (`nodes`) соединяются друг с другом, а также разберём **типы Service**. Эти знания помогут правильно выстраивать сетевую архитектуру, обеспечивая корректное и безопасное взаимодействие приложений.

## Сетевая топология

Чтобы понять, как устроена сеть в Kubernetes и как пользовательские запросы достигают конкретных подов, рассмотрим сетевую топологию **"снизу вверх"**.

### Node Network

Всё начинается с **сети, к которой подключены серверы кластера**. Назовём её **Node Network**. В рамках **Node Network** происходит:

- Общение **worker-узлов** (рабочих узлов) с управляющим слоем кластера.
- Взаимодействие **worker-узлов между собой**.
- Доступ **системного администратора** к серверам.

Node Network представляет собой **физическую** или (чаще) **виртуальную сеть**, объединяющую виртуальные машины, участвующие в кластере.  
Часто узлы имеют **только внутренний IP-адрес** и недоступны извне. Однако, если требуется, узлам можно назначить **внешний IP-адрес**.

---

### Network Namespace

Когда **контейнер** запускается в Kubernetes, он изолирован не только с точки зрения файловой системы и процессов, но и **с точки зрения сети**. Эта изоляция обеспечивается с помощью механизма **Linux Namespaces**.

#### Что такое Linux Namespaces?
**Linux Namespaces** — это технология ядра Linux, позволяющая изолировать ресурсы процессов.  
Для сетевых пространств имён (`Network Namespace`) это означает, что **контейнеры** могут иметь:

- **Собственные сетевые интерфейсы**.
- **Собственные IP-адреса**.
- **Изолированные таблицы маршрутизации**.

#### Как Network Namespace работает в Kubernetes?
- В **обычном контейнере** (например, Docker) у каждого контейнера **свой сетевой namespace**, то есть он полностью изолирован.
- В **Kubernetes изоляция работает на уровне подов**, а не отдельных контейнеров внутри них.
- Контейнеры, находящиеся **в одном поде**, **разделяют одно и то же сетевое пространство** (`Network Namespace`).
- Все контейнеры **используют один и тот же сетевой интерфейс** и **имеют один IP-адрес на под**.

Таким образом, **контейнеры внутри одного пода могут свободно взаимодействовать друг с другом**, как если бы они находились на одном сервере. Но при этом **поды изолированы друг от друга**.

# Pod Network в Kubernetes

**Pod Network** — это **виртуальная сеть**, которая обеспечивает взаимодействие между подами внутри кластера Kubernetes.  
При развертывании кластера администратор назначает **IP-адреса подам** в рамках **большой подсети**, которая **не связана** с **Node Network**.

## Как работает Pod Network?

- **Каждому рабочему узлу** выделяется **подсеть меньшего размера**, содержащая IP-адреса для подов, развернутых на этом узле.
- **Каждый под получает свой уникальный IP-адрес**, который **остаётся неизменным в течение всего его жизненного цикла**.

---

## Требования Kubernetes к сетевой связности подов

1. **Поды на одном узле могут взаимодействовать напрямую с подами на других узлах без NAT.**  
   Это называется **"flat network model"** (плоская модель сети).
2. **Системные сервисы узла (например, `kubelet`) могут напрямую взаимодействовать с подами на этом узле без NAT.**

### Пример взаимодействия подов

- **Pod A** с IP-адресом `10.0.0.2` находится на узле **Node1**.
- **Pod B** с IP-адресом `10.0.0.3` находится на узле **Node2**.

Они могут **напрямую обмениваться данными по сети** через свои IP-адреса, независимо от их физического размещения.

# Endpoints в Kubernetes

Поды в Kubernetes **динамически создаются и удаляются**, получая **произвольные IP-адреса**.  
Чтобы сервисы могли находить и взаимодействовать с подами **без привязки к конкретным IP-адресам**, Kubernetes использует сущность **Endpoints**.

## Как работают Endpoints?

**Endpoints** создаются **в паре с каждым сервисом** и **автоматически обновляются** при изменении списка подов.  
Когда под создаётся или удаляется, **список IP-адресов в Endpoints также изменяется**.


```sh
kubectl get svc/kube-dns -n kube-system
```
NAME       TYPE        CLUSTER-IP    EXTERNAL-IP   PORT(S)         AGE
kube-dns   ClusterIP   172.20.0.10   <none>        53/UDP,53/TCP   67d
```sh
kubectl get endpoints/kube-dns -n kube-system
```
NAME       ENDPOINTS                                                   AGE
kube-dns   10.1.101.179:53,10.1.101.8:53,10.1.101.179:53 + 1 more...   67d
- Таким образом, изолированные друг от друга поды могут взаимодействовать через IP-адреса, которые выделяются эндпоинтами.

```sh
kubectl get endpoints <имя-сервиса> -o yaml
```
```yaml
apiVersion: v1 # версия API
kind: Endpoints # тип Endpoints
metadata:
  annotations:
    endpoints.kubernetes.io/last-change-trigger-time: "2024-10-24T18:33:46Z"
  creationTimestamp: "2024-10-24T18:33:42Z"
  labels: # теги (лейблы)
    run: tomcat-pod
  name: tomcat-service # имя эндпоинта
  namespace: user64t32e1p60wkcc0 # текущий неймспейс
  resourceVersion: "11832502"
  uid: 63ea7969-bdd0-4302-9a33-9a762d23bf74
subsets:
- addresses:
  - ip: 10.112.129.113 # выделенный внутренний IP-адрес
    nodeName: cl1jepu1h9j5n3g00iug-uhyq # узел, на котором располагается под
    targetRef: # цель, на которую будет назначен этот IP-адрес
      kind: Pod
      name: tomcat-pod # имя пода, куда будет вести этот эндпоинт
      namespace: user64t32e1p60wkcc0 # неймспейс, где располагается под
      uid: 25399e74-113c-457e-9772-4bc73caf67f0
  ports: # порт на под, куда будут отправляться запросы
  - port: 8080
    protocol: TCP
```

# CNI (Container Network Interface) в Kubernetes

Обеспечение сетевой связности подов (их взаимодействие с любого рабочего узла и доступность через `kubectl`) — это задача **CNI-плагинов**.  
Эти компоненты управляют маршрутизацией трафика, создают сетевые интерфейсы и обеспечивают связь подов в **Pod Network**.  

Примеры популярных CNI-плагинов:  
✅ **Calico**  
✅ **Flannel**  
✅ **Weave**  

---

## Что такое CNI?

**CNI (Container Network Interface)** — это **спецификация** правил взаимодействия контейнеров по сети, а также механизм создания и удаления их **сетевых интерфейсов**.

Ключевые функции CNI:
- Создаёт и управляет **сетевыми интерфейсами для подов**.
- Настраивает **маршруты трафика** между узлами.
- Поддерживает **изолированные сетевые пространства** (Network Namespaces).
- Работает с **veth-интерфейсами** для связи контейнеров и узлов.

### Как работает CNI?

1. **Создаётся виртуальный коммутатор (Linux Bridge)** на каждом рабочем узле.
2. Поды **подключаются к нему** через виртуальные интерфейсы (`veth`).
3. В **локальную таблицу маршрутизации** добавляется маршрут до подсетей подов на других узлах.
4. При создании нового пода **ему автоматически назначается IP-адрес**, а другие поды узнают о нём через **DNS**.

Так работает встроенный плагин **kubenet** — базовый CNI-плагин Kubernetes.

---

## Безопасность и балансировка в CNI

**CNI-плагины** не только обеспечивают соединение подов, но и **улучшают безопасность и балансировку нагрузки**:

🔹 **Изоляция сетевых пространств**  
- Каждый под получает **свой сетевой namespace**.  
- Плагины (например, **Calico**) могут контролировать сетевые политики, запрещая или разрешая соединения между подами.

🔹 **Динамическое обновление маршрутизации**  
- Назначение **IP-адресов** новым подам.  
- **Flannel** и **Weave** создают **оверлейные сети**, обеспечивая связь подов независимо от их расположения.

🔹 **Балансировка нагрузки**  
- CNI-интеграция с Kubernetes обеспечивает **распределение трафика между подами на разных узлах**.
- Сервис Kubernetes остаётся доступным **даже при отказе отдельных узлов**.
# CNI в облачных провайдерах

В облачных платформах **CNI-плагины имеют отдельные настройки**, адаптированные под инфраструктуру провайдера.  
Это обеспечивает **плотную интеграцию Kubernetes с облачными сетевыми сервисами**.

---

## CNI в AWS: **AWS VPC CNI**

В AWS используется **AWS VPC CNI** — специальный CNI-плагин, интегрированный с сетью **AWS VPC (Virtual Private Cloud)**.

🔹 **Как это работает?**
- Каждый под получает **собственный IP-адрес** из диапазона **VPC**.
- Подключение подов **происходит на уровне AWS**, что упрощает маршрутизацию.
- Поды могут **напрямую взаимодействовать с сервисами AWS** (S3, RDS, Lambda).

🔹 **Преимущества AWS VPC CNI:**
- Не требует **оверлейных сетей** (в отличие от Flannel или Weave).
- Обеспечивает **низкую задержку** при сетевом взаимодействии.
- Полностью совместим с **AWS Security Groups и IAM-правилами**.

---

## CNI в Yandex Cloud: **Calico**

В **Yandex Cloud** используется **стандартный CNI-плагин Calico**, который обеспечивает **гибкую и безопасную сетевую конфигурацию**.

🔹 **Основные возможности Calico в Yandex Cloud:**
- **Поддержка IP-маршрутизации** и политики безопасности.
- **Маршрутизация пакетов между подами** через стандартные сетевые протоколы.
- **Сетевые политики**, позволяющие:
  - Ограничивать доступ между подами по **IP-адресам**.
  - Настраивать **внешние подключения** к подам.
- **Поддержка протокола BGP (Border Gateway Protocol)**, который:
  - Позволяет узлам кластера **обмениваться маршрутной информацией**.
  - Упрощает **масштабирование сети Kubernetes**.
# Service Network в Kubernetes

Мы обсудили поды и узлы, но это ещё не всё. **Service Network** — это **слой абстракции**, который позволяет **гибко настраивать доступ** к подам внутри кластера.  

**Service** в Kubernetes представляет собой **именованную абстракцию** для доступа к группе подов. Группа определяется с помощью **лейблов (labels)** — все поды, соответствующие селектору, становятся доступными через сервис.  

Service Network также **назначает IP-адреса сервисам**, которые обычно являются **виртуальными и внутренними**. Однако, если требуется, **сервисы можно сделать доступными извне**.

---

## Типы сервисов в Kubernetes

Kubernetes поддерживает несколько типов сервисов, определяющих способ доставки трафика до подов:

### 1️⃣ **ClusterIP** (сервис по умолчанию)

🔹 **Как работает?**  
- Выделяет **внутренний IP-адрес** из диапазона **Service Network**.  
- Доступен **только внутри кластера**.  
- Поды могут обращаться к сервису через **DNS-имя**.

🔹 **Как реализован?**  
- Использует **iptables** или **IPVS** для балансировки нагрузки.  
- `kube-proxy` на каждом узле **отслеживает сервисы** и добавляет правила NAT.

🔹 **Когда использовать?**  
✔ Внутренние сервисы, доступные только другим подам (например, базы данных).  

---

### 2️⃣ **NodePort** (доступ извне через узел)

🔹 **Как работает?**  
- Назначает **порт из диапазона 30000-32767** на каждом узле.  
- Доступ к приложению осуществляется через **<IP-адрес узла>:<номер порта>**.  
- Под капотом создаётся **ClusterIP**, а `kube-proxy` настраивает балансировку.

🔹 **Преимущества:**  
✔ Простое решение для доступа извне, **без дополнительных компонентов**.  

🔹 **Недостатки:**  
❌ **Ограничения по масштабированию** (уникальные порты на всех узлах).  
❌ **Ограниченная маршрутизация** (только **4 уровень OSI**, не управляет HTTP/HTTPS).  

🔹 **Когда использовать?**  
✔ Быстрый доступ к сервису **извне кластера**, если нет балансировщика.  

---

### 3️⃣ **LoadBalancer** (автоматический балансировщик)

🔹 **Как работает?**  
- Создаётся **внешний балансировщик** (если кластер работает в облаке).  
- Автоматически создаёт **NodePort и ClusterIP**.  
- Внешний балансировщик направляет трафик на **<IP-узла>:<NodePort>**.

🔹 **Преимущества:**  
✔ **Автоматическая интеграция с облачными провайдерами** (AWS, GCP, Yandex Cloud).  
✔ **Распределение нагрузки между узлами**.  

🔹 **Когда использовать?**  
✔ **Доступ к сервисам Kubernetes из интернета**.  

---
# Как работает сетевая инфраструктура в Kubernetes

Kubernetes использует **многоуровневую сетевую архитектуру**, которая позволяет подам взаимодействовать внутри кластера и обеспечивать доступ к внешним сервисам. Рассмотрим, как это работает **верхнеуровнево**.

---

## 1️⃣ Процесс создания сети для пода

1. **Создание пода**  
   - API-сервер отправляет информацию о новом поде на узел через **kubelet**.
2. **Инициализация сетевого интерфейса**  
   - `kubelet` вызывает **CNI-плагин**, который создаёт **сетевой интерфейс** для пода.
3. **Назначение IP-адреса**  
   - CNI-плагин выделяет **IP-адрес** из заранее настроенного пула узла или кластера.
   - IP-адрес пода **регистрируется в сетевых таблицах** узла.
4. **Создание сетевого пространства имён**  
   - CNI-плагин создаёт **виртуальные интерфейсы** и связывает их с подом.
   - Эти интерфейсы могут работать через **мосты (bridge)**, **тоннели (VXLAN)** или **BGP-маршрутизацию**.
5. **Обеспечение маршрутизации**  
   - Интерфейсы соединяются с **основной сетью узла**, что позволяет передавать трафик между узлами.

После завершения этих шагов под может отправлять и получать сетевые пакеты.

---

## 2️⃣ Ключевые механизмы сетевого взаимодействия

### 📌 VXLAN (Virtual Extensible LAN)
- Один из наиболее распространённых механизмов для **создания тоннелей между узлами**.
- Инкапсулирует пакеты **между подами** в отдельные сетевые пакеты.

### 📌 BGP (Border Gateway Protocol)
- Используется некоторыми **CNI-плагинами** (например, **Calico**).
- Позволяет узлам обмениваться **маршрутизируемыми префиксами**, что упрощает маршрутизацию трафика.

### 📌 Маршрутизация через узлы
- Каждый узел Kubernetes действует как **маршрутизатор** для запущенных на нём подов.
- Пакеты между подами на **разных узлах** передаются через узел, который знает, куда их направлять.

---

## 3️⃣ Основные модели сетевого взаимодействия контейнеров

### 🔹 **Bridge (мост)**
- Контейнеры подключаются к **виртуальному мосту** (например, `docker0` в Docker).
- Внутреннее взаимодействие между контейнерами **возможно по IP-адресам**.
- Для внешнего доступа требуется **проброс портов**.

### 🔹 **Host Network**
- Контейнер **использует IP-адрес** самого узла.
- Контейнеры не изолированы и могут **напрямую использовать сетевые интерфейсы** хоста.

### 🔹 **None (отсутствие сети)**
- Контейнер не имеет **никакого сетевого взаимодействия**.
- Используется, если контейнеру **не нужна сеть**.

### 🔹 **Overlay Network**
- Создаёт **виртуальную сеть поверх физической сети**.
- Позволяет контейнерам **на разных узлах** общаться друг с другом, как будто они находятся в одной локальной сети.
- Kubernetes использует **оверлейные сети через CNI-плагины**.

# Service Discovery в Kubernetes

## 1 Что такое Service Discovery?

До появления Kubernetes сервисы обнаруживались **клиентским способом**:
- Приложение обращалось к **реестру сервисов**.
- Выбирало подходящий экземпляр с помощью **агентов обнаружения**.

В Kubernetes этот процесс **автоматизирован**:
- **Регистрация сервисов** происходит автоматически.
- **Балансировка нагрузки** встроена в платформу.
- Kubernetes создаёт **виртуальные эндпоинты**, которые динамически маршрутизируют трафик к подам.

Благодаря этому взаимодействие сервисов **становится проще и эффективнее**.

---

##  Как приложения находят друг друга в Kubernetes?

Допустим, у нас есть **фронтенд** и **бэкенд**, и нам нужно настроить их сетевое взаимодействие.  
Самый простой способ — **использовать IP-адреса подов**, но у этого метода есть недостатки:
- **Поды могут пересоздаваться**, меняя IP-адрес.
- **Ручное управление IP-адресами** в большом кластере неэффективно.

Поэтому Kubernetes использует **динамическое выделение IP-адресов** и DNS для маршрутизации трафика.

---
##  Как работает Service Discovery в Kubernetes?

Kubernetes использует **DNS и механизмы балансировки нагрузки**.

✅ **DNS-обнаружение**  
- Kubernetes создаёт **DNS-запись** для каждого сервиса.
- Приложение может обращаться к сервису через его **DNS-имя**.

✅ **Обновление Endpoints**  
- Kubernetes **динамически обновляет список подов** в сервисе.
- Балансировка происходит через **ClusterIP или другие типы сервисов**.

✅ **Маршрутизация трафика**  
- Kubernetes направляет трафик к **доступным подам**, даже если их IP изменился.

---

# Session Affinity и доступ к сервисам в Kubernetes

## 1️⃣ Session Affinity (Sticky Sessions)

**Session Affinity** (привязка сессии) позволяет **направлять трафик от одного клиента на один и тот же под**.  
Это полезно, если приложение **сохраняет состояние пользователя** и требует последовательного взаимодействия с тем же сервером.

🔹 **Как это работает?**
- Kubernetes запоминает, какой **под обрабатывал запрос клиента**.
- Все последующие запросы от этого клиента будут **направляться в тот же под**, пока он доступен.
- Основано на **IP-адресе клиента**.

🔹 **Как настроить?**  
Добавьте параметр `sessionAffinity: ClientIP` в манифест `Service`:

```yaml
apiVersion: v1
kind: Service
metadata:
  name: backend
spec:
  selector:
    app: backend
  ports:
    - protocol: TCP
      port: 80 
      targetPort: 80
  type: ClusterIP  
  sessionAffinity: ClientIP 
```

## 🔹 Недостатки Session Affinity
❌ При отказе пода клиент теряет сессию.  
❌ Возможна неравномерная нагрузка, если некоторые клиенты генерируют много трафика.  

---

## 2️⃣ Доступ к сервисам за пределами кластера

Для взаимодействия сервисов **frontend ↔ backend** или доступа извне можно использовать:  
✅ **NodePort**  
✅ **LoadBalancer**  
✅ **Ingress**  

---

### 🔹 NodePort (открывает порт на узле)
- Открывает **статический порт** на каждом узле Kubernetes (`30000-32767`).
- `kube-proxy` автоматически **перенаправляет трафик** на поды.

#### 🔹 Недостатки:
❌ **Нагрузка не балансируется** между узлами.  
❌ **Нужно знать IP-адрес узла**.  

---

### 🔹 LoadBalancer (автоматический балансировщик)
- **Создаёт балансировщик нагрузки** в облаке (AWS, GCP, Yandex Cloud).
- **Автоматически управляет масштабированием и трафиком**.
- Поддерживает **статический внешний IP**.

#### 🔹 Особенность для bare metal
❗ На физических серверах (не в облаке) **нужно установить MetalLB**,  
чтобы `LoadBalancer` получал **IP-адрес**.

---

## 3️⃣ Ingress — гибкое управление HTTP/HTTPS-трафиком

**Ingress** управляет **внешним доступом к сервисам** внутри кластера,  
используя **централизованные правила маршрутизации**.
```yaml
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: backend-ingress
spec:
  ingressClassName: nginx # указывает, что используется Ingress NGINX. В кластере может быть несколько Ingress-контроллеров
  rules:
  - host: example.com  # домен, на который будет приходить трафик
    http:
      paths:
      - path: / # так как Ingress работает с HTTP, мы можем управлять трафиком, учитывая путь. Директив path может быть несколько, тогда в зависимости от запроса трафик будет направлен на соответствующий сервис
        pathType: Prefix
        backend:
          service:
            name: backend # имя сервиса, на которое будет ссылаться Ingress
            port: 
              number: 80 # порт сервиса, на который будет ссылаться Ingress
```              
### 🔹 Как работает Ingress?
1. В кластере устанавливается **Ingress-контроллер** (например, **NGINX Ingress Controller**).
2. Контроллер получает **запрос через LoadBalancer**.
3. В зависимости от **конфигурации Ingress** он **направляет трафик** в нужный сервис.

### 🔹 Преимущества Ingress
✔ **Гибкое управление HTTP/HTTPS-трафиком**.  
✔ **Один балансировщик** для всех сервисов вместо отдельных **NodePort/LoadBalancer**.  
✔ **Поддержка TLS (HTTPS)**.  

---

### 🔹 Популярные Ingress-контроллеры

✅ **NGINX Ingress Controller** (самый распространённый).  
✅ **Traefik**.  
✅ **HAProxy**.  

---

## 4️⃣ Итог

| Способ доступа  | Доступен извне? | Балансировка нагрузки | Когда использовать? |
|---------------|----------------|----------------------|-------------------|
| **ClusterIP**  | ❌ Только внутри кластера | 🔄 Через kube-proxy | Внутренние сервисы |
| **NodePort**   | ✅ Через IP узла + порт | ❌ Нет | Тестирование, небольшие кластеры |
| **LoadBalancer** | ✅ Автоматический IP | ✅ Да, через облачного провайдера | Облачные сервисы |
| **Ingress** | ✅ HTTP/HTTPS | ✅ Да, гибкая маршрутизация | Веб-приложения, API |
# ExternalName и дополнительные средства Service Discovery в Kubernetes

## 1️⃣ ExternalName Service

**ExternalName** — это специальный тип `Service`, который **перенаправляет запросы на внешний DNS-ресурс**,  
а не на IP-адрес внутри кластера.

🔹 **Когда использовать ExternalName?**
- Интеграция **внешних API и сервисов** (например, облачная база данных).
- Доступ к **внешним ресурсам по DNS** без использования **жёстко заданных IP-адресов**.

🔹 **Пример манифеста ExternalName**
```yaml
apiVersion: v1
kind: Service
metadata:
  name: external-service
spec:
  type: ExternalName
  externalName: example.com
```
# ExternalName и дополнительные средства Service Discovery

Теперь внутри кластера `example.com` доступен по имени сервиса **external-service**.

---

## ✅ Преимущества:

- Не требует **жёсткого задания IP-адресов**.
- Простая интеграция с **облачными сервисами**.
- Позволяет использовать **стандартный Kubernetes DNS**.

## ❌ Ограничения:

- Работает **только на DNS-уровне** (не маршрутизирует трафик через `kube-proxy`).
- **Не поддерживает балансировку** — если `example.com` отдаёт несколько IP, Kubernetes **не управляет** этим.

---

## 2️⃣ Дополнительные средства для Service Discovery

**Kubernetes DNS** отлично работает **в одном кластере**.  
Но если сервисы расположены **в нескольких кластерах** или в **гибридных окружениях**,  
понадобятся **дополнительные инструменты Service Discovery**.

---

### 🔹 Ambassador (API-шлюз для Kubernetes)

**Ambassador** — это **API-шлюз с открытым исходным кодом**,  
разработанный специально для **Kubernetes и облачных приложений**.

### 🔹 Возможности Ambassador:

- 📡 **Service Discovery и маршрутизация** — автоматически обнаруживает сервисы в Kubernetes и направляет к ним HTTP-запросы.
- 🔄 **API Gateway** — управляет API-запросами через **Envoy Proxy**.
- 🔒 **Безопасность** — поддержка **TLS, OAuth, аутентификации**.

### **Когда использовать Ambassador?**
✅ Когда **нужно маршрутизировать API-запросы** через шлюз.  
✅ Для **гибкой балансировки нагрузки** и **безопасного API-доступа**.

---

### 🔹 Consul (Service Discovery и управление конфигурациями)

**Consul** от HashiCorp — это **многофункциональная платформа Service Discovery**,  
которая работает **как в Kubernetes, так и за его пределами**.

### 🔹 Ключевые функции Consul:

- 🔍 **Service Discovery** — автоматическая **регистрация и поиск сервисов** через DNS или HTTP API.
- 🏥 **Health Checks** — встроенные **проверки состояния сервисов**.
- 🌍 **Интеграция с Kubernetes** — позволяет **синхронизировать кластер Kubernetes с внешними сервисами**.

### **Когда использовать Consul?**
✅ Когда **нужно объединить сервисы в Kubernetes и за его пределами**.  
✅ Для **автоматического управления отказоустойчивостью** (исключение нерабочих сервисов).  

---

## 3️⃣ Итог

**Встроенные возможности Kubernetes Service Discovery** значительно **упрощают управление микросервисами**:

- **DNS-обнаружение сервисов** внутри кластера.
- **Балансировка нагрузки** через `kube-proxy`.
- **Автоматическая маршрутизация** через `Service` и `Ingress`.

Но если **сервисы работают в нескольких кластерах** или в **разнородной среде**,  
то **Ambassador и Consul** помогают **объединять распределённые системы** и **упрощают Service Discovery**. 🚀
